{"cells":[{"cell_type":"markdown","source":["# Register an inference spark environment\nCustom [image](https://github.com/Azure/mmlspark/blob/master/tools/docker/minimal/Dockerfile) with Spark 3.0.1 from mmlspark."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f5f5766-bd80-4dd9-aada-5df1966e8759"}}},{"cell_type":"markdown","source":["## Get the Workspace"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"079c1625-84a0-4c1c-913b-b9cbe2ca3624"}}},{"cell_type":"code","source":["import azureml\nfrom azureml.core import Workspace\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core.environment import Environment\n\nworkspace_name = '<YOUR-WORKSPACE-NAME>'\nresource_group = '<YOUR-RESOURCE-GROUP>'\nsubscription_id = '<YOUR-SUBSCRIPTION-ID>'\n\ndef get_workspace(workspace_name, resource_group, subscription_id):\n  svc_pr = ServicePrincipalAuthentication(\n      tenant_id = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"tenant-id\"),\n      service_principal_id = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"cliente-id-custom-role\"),\n      service_principal_password = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"cliente-secret-custom-role\"))\n\n  workspace = Workspace.get(name = workspace_name,\n                            resource_group = resource_group,\n                            subscription_id = subscription_id,\n                            auth=svc_pr)\n  \n  return workspace\n\nworkspace = get_workspace(workspace_name, resource_group, subscription_id)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfc096f1-909b-417b-8ca8-fb5999df7b77"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create the environment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2bcb747-c36d-4f3f-ad84-3a24b832efc8"}}},{"cell_type":"code","source":["from azureml.core.environment import Environment\nfrom azureml.core.webservice import LocalWebservice\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig, Model\n\n# BASE IMAGE from https://github.com/Azure/mmlspark/blob/master/tools/docker/minimal/Dockerfile \ndockerfile = \"\"\"\nFROM ubuntu:16.04\n\nARG SPARK_VERSION=3.0.1\nRUN apt-get -qq update && apt-get -qq -y install curl bzip2 \\\n    && curl -sSL https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -o /tmp/miniconda.sh \\\n    && bash /tmp/miniconda.sh -bfp /usr/local \\\n    && rm -rf /tmp/miniconda.sh \\\n    && conda install -y python=3 \\\n    && conda update conda \\\n    && apt-get install default-jre -y \\\n    && conda install -c conda-forge pyspark=${SPARK_VERSION} \\\n    && apt-get -qq -y remove curl bzip2 \\\n    && apt-get -qq -y autoremove \\\n    && apt-get autoclean \\\n    && rm -rf /var/lib/apt/lists/* /var/log/dpkg.log \\\n    && conda clean --all --yes\n\nENV PATH /opt/conda/bin:$PATH\nENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64\n\"\"\"\n\n# Create an environment to be able to customize our dependencies\nmy_spark_env = Environment('spark-env-custom')\n\n# Add custom libs from PyPi\nmy_spark_env.python.conda_dependencies.add_pip_package(\"azureml-defaults\")\nmy_spark_env.python.conda_dependencies.add_pip_package(\"mlflow\")\nmy_spark_env.python.conda_dependencies.add_pip_package(\"pyspark\")\n\n# Now we can indicate we will use our custom Base Image\nmy_spark_env.docker.base_image = None\nmy_spark_env.docker.base_dockerfile = dockerfile\n\n# It's very important to use this parameter\nmy_spark_env.inferencing_stack_version='latest'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e4b865e-215f-43ff-923d-ae9be5f2486b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Register the environment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dca9cae-db24-4e27-a636-d89116cfde32"}}},{"cell_type":"code","source":["my_spark_env.register(workspace)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e38e7e6-7b4b-414c-b3db-48e32ac38a86"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3.6 - AzureML","language":"python","name":"python3-azureml"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.9","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"register-spark-environment","dashboards":[],"language":"python","widgets":{},"notebookOrigID":177887194859226}},"nbformat":4,"nbformat_minor":0}
