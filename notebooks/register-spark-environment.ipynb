{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7f5f5766-bd80-4dd9-aada-5df1966e8759","showTitle":false,"title":""}},"source":["# Register an inference spark environment\n","Custom [image](https://github.com/Azure/mmlspark/blob/master/tools/docker/minimal/Dockerfile) with Spark 3.0.1 from mmlspark."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"079c1625-84a0-4c1c-913b-b9cbe2ca3624","showTitle":false,"title":""}},"source":["## Get the Workspace"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"dfc096f1-909b-417b-8ca8-fb5999df7b77","showTitle":false,"title":""}},"outputs":[],"source":["import azureml\n","from azureml.core import Workspace\n","from azureml.core.authentication import ServicePrincipalAuthentication\n","from azureml.core.environment import Environment\n","\n","workspace_name = '<YOUR-WORKSPACE-NAME>'\n","resource_group = '<YOUR-RESOURCE-GROUP>'\n","subscription_id = '<YOUR-SUBSCRIPTION-ID>'\n","\n","def get_workspace(workspace_name, resource_group, subscription_id):\n","  svc_pr = ServicePrincipalAuthentication(\n","      tenant_id = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"tenant-id\"),\n","      service_principal_id = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"cliente-id-custom-role\"),\n","      service_principal_password = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"cliente-secret-custom-role\"))\n","\n","  workspace = Workspace.get(name = workspace_name,\n","                            resource_group = resource_group,\n","                            subscription_id = subscription_id,\n","                            auth=svc_pr)\n","  \n","  return workspace\n","\n","workspace = get_workspace(workspace_name, resource_group, subscription_id)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e2bcb747-c36d-4f3f-ad84-3a24b832efc8","showTitle":false,"title":""}},"source":["## Create the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4e4b865e-215f-43ff-923d-ae9be5f2486b","showTitle":false,"title":""}},"outputs":[],"source":["from azureml.core.environment import Environment\n","from azureml.core.webservice import LocalWebservice\n","from azureml.core.conda_dependencies import CondaDependencies\n","from azureml.core.model import InferenceConfig, Model\n","\n","# BASE IMAGE from https://github.com/Azure/mmlspark/blob/master/tools/docker/minimal/Dockerfile \n","dockerfile = \"\"\"\n","FROM mcr.microsoft.com/azureml/minimal-ubuntu18.04-py37-cpu-inference:latest\n","\n","USER root:root\n","\n","ARG SPARK_VERSION=3.2.0\n","RUN mkdir -p /usr/share/man/man1 && \\\n","    apt-get -qq update && apt-get \\\n","    -qq upgrade && apt-get -qq -y install curl bzip2 && \\\n","    curl -sSL https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -o /tmp/miniconda.sh && \\\n","    bash /tmp/miniconda.sh -bfp /usr/local && rm -rf /tmp/miniconda.sh && \\\n","    conda install -y python=3 && \\\n","    conda update conda && \\\n","    apt-get install openjdk-8-jre -y && \\\n","    conda install -c conda-forge pyspark=${SPARK_VERSION} && \\\n","    apt-get -qq -y remove curl bzip2 && \\\n","    apt-get -qq -y autoremove && \\\n","    apt-get autoclean && \\\n","    rm -rf /var/lib/apt/lists/* /var/log/dpkg.log && \\\n","    conda clean --all --yes\n","\n","RUN pip install mlflow pyspark pandas azureml-defaults\n","\n","ENV PATH /opt/conda/bin:$PATH\n","ENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64\n","\"\"\"\n","\n","# Create an environment to be able to customize our dependencies\n","my_spark_env = Environment('spark-env-custom')\n","\n","# Now we can indicate we will use our custom Base Image\n","my_spark_env.docker.base_image = None\n","my_spark_env.docker.base_dockerfile = dockerfile\n","my_spark_env.python.user_managed_dependencies = True\n","\n","# It's very important to use this parameter\n","my_spark_env.inferencing_stack_version='latest'"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8dca9cae-db24-4e27-a636-d89116cfde32","showTitle":false,"title":""}},"source":["## Register the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3e38e7e6-7b4b-414c-b3db-48e32ac38a86","showTitle":false,"title":""}},"outputs":[],"source":["my_spark_env.register(workspace)"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookName":"register-spark-environment","notebookOrigID":177887194859226,"widgets":{}},"kernelspec":{"display_name":"Python 3.6 - AzureML","language":"python","name":"python3-azureml"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}
