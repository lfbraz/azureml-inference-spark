{"cells":[{"cell_type":"markdown","source":["# Deploy a spark model from an environment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"903bd2a1-b327-4c4a-898a-5a4c1da05b86"}}},{"cell_type":"markdown","source":["## First get the Workspace and register the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"726d8467-56d7-4ec3-8f7d-1add73ffd2f7"}}},{"cell_type":"code","source":["import azureml\nfrom azureml.core import Workspace\nimport mlflow.azureml\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core.environment import Environment\n\nworkspace_name = '<YOUR-WORKSPACE-NAME>'\nresource_group = '<YOUR-RESOURCE-GROUP>'\nsubscription_id = '<YOUR-SUBSCRIPTION-ID>'\n\ndef get_workspace(workspace_name, resource_group, subscription_id):\n  svc_pr = ServicePrincipalAuthentication(\n      tenant_id = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"tenant-id\"),\n      service_principal_id = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"cliente-id-custom-role\"),\n      service_principal_password = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"cliente-secret-custom-role\"))\n\n  workspace = Workspace.get(name = workspace_name,\n                            resource_group = resource_group,\n                            subscription_id = subscription_id,\n                            auth=svc_pr)\n  \n  return workspace\n\nworkspace = get_workspace(workspace_name, resource_group, subscription_id)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21ed3d85-2b91-4724-a1bf-cbd5310107bc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Entry script"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f7186e3-681f-4405-b283-63853eb99655"}}},{"cell_type":"code","source":["%%writefile /tmp/score.py\n\nimport os\nimport pickle\nimport json\nimport time\nfrom mlflow.pyfunc import load_model\nimport pandas as pd\nimport traceback\n\n# Called when the deployed service starts\ndef init():\n    global model\n    # Get the path where the deployed model can be found.\n    model_path = os.getenv('AZUREML_MODEL_DIR') + '/your-model'\n    # load models\n    model = load_model(model_path)\n\ndef run(data):\n    try:\n        data = pd.DataFrame(json.loads(data))\n        prediction = model.predict(data)\n        return {\"prediction\": str(prediction)}\n    except Exception as e:\n        traceback.print_exc()\n        error = str(e)\n        return error"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09100978-b340-41b8-9668-612350f8bcd1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Register a new model version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1ba080f-43f5-46ce-94ad-dc9361e23f63"}}},{"cell_type":"code","source":["from azureml.core.model import Model\n\nmodel_name = 'model-spark'\nmodel_path = '/dbfs/mnt/models/your-model'\nmodel_description = 'My spark model'\n\nmodel_azure = Model.register(model_path=model_path,\n                             model_name=model_name,\n                             description=model_description,\n                             workspace=workspace,\n                             tags={'Framework': \"Spark\"})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13f88839-d812-4ecc-9989-5a0bdf2847dc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Define an inference config"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d37c91ec-10cb-449f-9260-6c2afb70a084"}}},{"cell_type":"code","source":["from azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nmy_spark_env = Environment.get(name='spark-env-custom', workspace=workspace)\n\ninference_config = InferenceConfig(entry_script=\"/tmp/score.py\", environment=my_spark_env)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f39f2217-5979-4fdb-a899-ee1e5ccb53b9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Deploy the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26e60fdb-4f69-4a44-b344-53f3b3823243"}}},{"cell_type":"markdown","source":["### ACI"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11c712e9-0e41-413b-8c0b-4059aff5ef4c"}}},{"cell_type":"code","source":["from azureml.core.model import InferenceConfig\nfrom azureml.core.webservice import AciWebservice, Webservice\n\nservice_name = 'api-model-dev'\n\naci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1, description=\"This is a spark serving example.\")\n \nservice = Model.deploy(name=service_name, deployment_config=aci_config, models=[model_azure], inference_config=inference_config, workspace=workspace, overwrite=True)\nservice.wait_for_deployment(show_output=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19ae1456-cf58-4dd8-a044-5cde50fccf74"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"deploy-from-environment","dashboards":[],"language":"python","widgets":{},"notebookOrigID":177887194859234}},"nbformat":4,"nbformat_minor":0}
