{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c5aa13f5-c8ba-4dba-8a4b-7927173e665f","showTitle":false,"title":""}},"source":["# Spark classification model\n","Here we will use the [Customer Sample Dataset]('./data') to train a very simple **Classification Model** using MLFlow(https://mlflow.org/) to fit a native spark model.\n","\n","This model will have three stages: \n","* [StringIndexer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html) to map \"UF\" category field to a label index\n","* [VectorAssembler](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all the columns used to train the model in a vector column \n","* [LogisticRegression Model](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegressionModel.html) to fit a Logistic Regression model to predict the Churn"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3e543982-0963-4bab-bc7e-74b0fad9c396","showTitle":false,"title":""}},"source":["Read the Customer sample Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4659802a-3ea0-4100-bb45-5f2a198f7dc0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["Customer = spark.read.parquet('/dbfs/Dataset/Customer')"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"eef21a5b-e0ae-4142-bc48-5aa485ee5fb7","showTitle":false,"title":""}},"source":["Select the columns to be used in the training"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0226d2d1-827e-461b-aec9-5a3b0eab63ef","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["numeric_columns = ['Idade',\n"," 'RendaMensal',\n"," 'PercentualUtilizacaoLimite',\n"," 'QtdTransacoesNegadas',\n"," 'AnosDeRelacionamentoBanco',\n"," 'JaUsouChequeEspecial',\n"," 'QtdEmprestimos',\n"," 'NumeroAtendimentos',\n"," 'TMA',\n"," 'IndiceSatisfacao',\n"," 'Saldo',\n"," 'CLTV'\n","]\n","\n","columns = list(numeric_columns)\n","columns.append('UF')\n","columns.append('Churn')"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"16551703-e5a7-4c02-af02-56de76e0897f","showTitle":false,"title":""}},"source":["Create the train dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b7c4f3f1-45a0-4bf7-9aeb-ff6abe03c947","showTitle":false,"title":""}},"outputs":[],"source":["dataset = Customer.select(columns)\n","display(dataset)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fadd09ed-a686-4acb-84ff-519c10ca1549","showTitle":false,"title":""}},"source":["## Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9a0e615c-a2d3-49b4-94a6-7db7a1fba569","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["from pyspark.ml import Pipeline\n","from pyspark.ml.feature import StringIndexer, VectorAssembler\n","from pyspark.ml.classification import LogisticRegression\n","\n","indexer = StringIndexer(inputCol='UF', outputCol=\"UF_Index\")\n","\n","assembler_columns = list(numeric_columns)\n","assembler_columns.append('UF_Index')\n","assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"features\")\n","\n","lr = LogisticRegression(maxIter=10, regParam=0.01, labelCol='Churn')\n","\n","pipeline = Pipeline(stages=[indexer, assembler, lr])\n","model = pipeline.fit(dataset)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"18fc2a13-c0a3-49ef-9db7-45bb28a12734","showTitle":false,"title":""}},"source":["## Log the Model using MLFLow\n","Finally we can track the model using [MLFlow](https://mlflow.org/) Platform. Feel free to extend the capabilities (e.g using an experiment).\n","With the fitted model we can use this run to deploy it to a inference endpoint."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e219b62b-67ee-4e02-a6ea-821a57c77a5d","showTitle":false,"title":""}},"outputs":[],"source":["import mlflow\n","import mlflow.spark\n","\n","with mlflow.start_run():\n","    print(f'Your run_id:{mlflow.active_run().info.run_id}. Please use it to load and deploy your model')\n","    mlflow.spark.log_model(spark_model=model, artifact_path='model') "]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f91e9b1b-afcd-4a7b-a73c-9d39a6e8ed0a","showTitle":false,"title":""}},"source":["### Predict\n","Send a payload to test the model. We don't need to care about the transformations because the data will follow all the stages we have defined in the pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"747f2cf0-8cf7-4e4f-b3de-8f27a00e6034","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[36]: [1.0]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[36]: [1.0]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Load the model using PyFunc\n","loaded_model = mlflow.pyfunc.load_model(f'runs:/a2e0a0ce09cc47ed9ee1bcde7874fb83/model')\n","\n","import json\n","import pandas as pd\n","\n","data = '{ \\\n","    \"Idade\": [22], \\\n","      \"RendaMensal\": [1000], \\\n","      \"PercentualUtilizacaoLimite\": [1], \\\n","      \"QtdTransacoesNegadas\": [100], \\\n","      \"AnosDeRelacionamentoBanco\": [12], \\\n","      \"JaUsouChequeEspecial\": [0], \\\n","      \"QtdEmprestimos\": [1], \\\n","      \"NumeroAtendimentos\": [100], \\\n","      \"TMA\": [300], \\\n","      \"IndiceSatisfacao\": [1], \\\n","      \"Saldo\": [6438], \\\n","      \"CLTV\": [71], \\\n","      \"UF\": [\"SP\"] \\\n"," } '\n","\n","data = pd.DataFrame(json.loads(data))\n","\n","predictions = loaded_model.predict(data)\n","predictions"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"train-spark-classification-model","notebookOrigID":1951359453673209,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
